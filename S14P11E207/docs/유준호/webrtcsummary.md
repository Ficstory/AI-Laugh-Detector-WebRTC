# WebRTC & openVidu v3 — 두 블로그 “전체 내용 포함” 상세 정리 (A4 약 8p 분량)

> 본 문서는 아래 2개 글의 **모든 핵심 서술 요소(용어/흐름/예시/주의사항/튜토리얼 단계)**를 빠짐없이 포함하여, 실무 적용 관점에서 **더 상세히 풀어서** 재구성한 정리본입니다.  
> - (1) 용뇽 개발 노트: *“webRTC란 무엇일까요? 주요 개념들 쉽게 설명 해드립니다.”*  
> - (2) 백엔드로 살아남기: *“간단하게 알아보는 openVidu v3”*

---

## 출처 (원문 링크)

- WebRTC 개념: `https://yong-nyong.tistory.com/99`
- openVidu v3 정리/튜토리얼: `https://savingbe.tistory.com/22`


---

# 1. WebRTC 전체 개요

## 1.1 WebRTC란?

WebRTC(Web Real-Time Communication)는 **웹 브라우저(및 앱)에서 별도의 플러그인이나 외부 프로그램 설치 없이** 실시간 통신을 가능하게 해주는 오픈소스 기반 기술(표준 API)이다.  
핵심은 다음 3가지 사용 형태로 요약된다.

- **화상 채팅(비디오/오디오)**
- **음성 통화(오디오)**
- **데이터 전송(파일 공유, 채팅 데이터 등)**

원문에서는 WebRTC가 2011년 Google 개발자들에 의해 탄생한 기술이며, Google Meet/Discord/Zoom 등 실서비스에서도 활용된다고 소개한다. 또한 “서버 없이 P2P로 직접 연결해 데이터를 주고받을 수 있다”는 점이 WebRTC의 인상적인 특성으로 강조된다.  
다만, **완전한 의미의 ‘서버 0’은 아니다.** 연결을 성사시키기 위해서는 **시그널링(Signaling)** 과정이 필요하고, 이 과정에서는 중계/교환 역할을 수행하는 서버가 필요하다는 점을 함께 명시한다.

## 1.2 “개방형 웹 표준”으로서의 의미

원문은 WebRTC가 **개방형 웹 표준(Open Web Standard)**에 기반하여, 주요 브라우저에서 **일반 JavaScript API로 제공**된다고 설명한다.  
또한 네이티브(모바일/데스크탑)에서도 동일한 기능을 제공하는 라이브러리를 사용할 수 있다고 정리한다.

> 실무적으로 이해하면:  
> - 브라우저: WebRTC API(예: `RTCPeerConnection`, `getUserMedia` 등)를 통해 구현  
> - 네이티브: iOS/Android SDK 레벨에서 WebRTC 스택을 활용한 구현 가능

---

# 2. WebRTC 핵심 용어 4종: Signaling / SDP / ICE / STUN·TURN

원문은 WebRTC를 이해하기 위해 반드시 알아야 하는 용어로 다음 4가지를 제시한다.

- 시그널링(Signaling)
- SDP(Session Description Protocol)
- ICE(Interactive Connectivity Establishment)
- STUN/TURN

이 절에서는 원문의 설명을 **그대로 포함**하면서, 실무자가 구현 시 어떤 의미로 연결되는지까지 확장한다.

---

## 2.1 시그널링(Signaling)

### 2.1.1 정의(원문 요지)
원문에서는 P2P 연결을 위해 “중계 서버가 필요하다”고 언급하고,  
**서로를 찾고 연결을 시작하는 과정**이 바로 **시그널링(Signaling)**이라고 설명한다.

### 2.1.2 비유(원문 요지)
시그널링은 마치 “소개팅 주선자”처럼, 양쪽에 서로의 정보를 전달해 주는 역할이라고 비유한다.  
즉, 실제 미디어(영상/음성)가 이 서버를 통해 계속 흐르는 것이 아니라, **연결 성립을 위한 ‘소개/교환’ 채널**로 이해하면 된다.

### 2.1.3 구현은 자유(원문 요지)
원문은 시그널링이 “서버를 통해 이루어지며, 어떻게 구현할지는 자유”라고 말한다.  
대표적으로:

- WebSocket
- HTTP(REST)
- 기타 메시징/큐 등

> 실무 팁(확장):  
> 시그널링은 **상태 동기화**가 중요하다.  
> - Offer/Answer 순서  
> - ICE Candidate가 언제/어떻게 도착하는지  
> - 재접속/룸 재진입 시 세션 정리  
> 를 안정적으로 처리하지 않으면 “연결이 되다가 끊기거나”, “특정 환경에서만 실패하는” 문제로 이어진다.

---

## 2.2 SDP(Session Description Protocol)

### 2.2.1 정의(원문 요지)
SDP는 “무엇으로 소통할지, 어떻게 소통할지”에 대한 정보가 담긴 양식이라고 설명한다.  
원문 비유대로 SDP는 **자기소개서**에 가깝다.

- 나는 어떤 오디오/비디오를 지원하는지
- 어떤 코덱을 사용할 수 있는지
- 어떤 네트워크/미디어 속성을 갖는지  
등이 포함된다.

### 2.2.2 Offer / Answer(원문 요지)
연결을 시작하는 쪽이 **Offer SDP**를 만들고, 받는 쪽이 **Answer SDP**로 응답한다.  
이 과정에서 지원 코덱/해상도 등의 협상이 이루어져 최적 설정이 결정된다.

### 2.2.3 SDP 예시(원문 포함)
원문에 포함된 SDP 구조 예시는 아래와 같다.

```sdp
v=0
o=- 1234567890 1234567890 IN IP4 127.0.0.1
s=-
t=0 0
a=group:BUNDLE audio video
m=audio 9 UDP/TLS/RTP/SAVPF 111
c=IN IP4 0.0.0.0
a=rtcp:9 IN IP4 0.0.0.0
a=ice-ufrag:someufrag
a=ice-pwd:someicepwd
a=fingerprint:sha-256 somefingerprint
...
```

원문은 “이 SDP를 가지고 피어끼리 알아서 협상을 하므로 개발자가 줄마다 외울 필요는 없다”고 말한다.  
즉, 구현자는 **SDP의 의미를 ‘개념적으로’ 이해**하고, 협상 결과를 보는 정도로 접근해도 충분하다는 메시지다.

> 실무적 확장:  
> 물론 문제 해결 단계에서는 SDP를 “읽을 줄” 알아야 한다.  
> - 코덱 협상 실패(브라우저/기기 호환성)  
> - ICE/DTLS 관련 설정 오류  
> - 특정 네트워크에서만 실패  
> 같은 경우 SDP/ICE 로그 분석이 필요하다.

---

## 2.3 ICE(Interactive Connectivity Establishment)

### 2.3.1 정의(원문 요지)
원문은 ICE를 “나한테 연락하려면 이 주소로 연락해!”라고 알려주는 과정으로 설명한다.  
즉, 두 피어가 최적의 통신 경로를 찾기 위해 사용하는 프레임워크다.

### 2.3.2 왜 ICE가 필요한가?
원문은 ICE의 핵심 역할을 다음처럼 설명한다.

- NAT(Network Address Translation) / 방화벽 등 **네트워크 장벽을 극복**
- 직접 연결 가능성을 높이는 중요한 역할 수행

### 2.3.3 ICE 동작 단계(원문 요지)
원문이 제시하는 단계는 아래와 같다.

- **연결 후보 수집**: 가능한 모든 연결 방법(후보)을 찾아냄  
- **연결성 검사**: 각 후보에 대해 실제 연결 가능 여부를 테스트  
- **우선순위 결정**: 가장 효율적인 경로(지연시간이 짧은 경로 선호) 선택  
- **NAT/방화벽 통과**: 다양한 네트워크 환경에서 연결을 가능하게 함

### 2.3.4 ICE가 시도하는 경로 우선순위(원문 포함)
원문은 ICE가 지연이 짧은 경로를 찾기 위해 아래 옵션을 순서대로 시도한다고 제시한다.

1. **직접 UDP 연결, 사설 IP**  
   - (이 경우에만) STUN 서버가 피어의 네트워크 연결 주소를 찾는 데 사용됨  
2. **HTTP 포트를 통한 직접 TCP 연결**  
3. **HTTPS 포트를 통한 직접 TCP 연결**  
4. **relay/TURN 서버를 통한 간접 연결**  
   - 직접 연결이 실패한 경우(예: NAT 통과를 차단하는 방화벽 뒤에 한 피어가 있는 경우)

> 확장(실무 관찰):  
> 실제 운영에서는 “4번(TURN relay)” 비중이 생각보다 크다.  
> 회사망, 호텔/공항 와이파이, 일부 통신사 NAT, 기업 VPN 등이 겹치면 TURN을 타는 경우가 증가한다.  
> TURN은 비용·성능 영향이 크므로, 운영 설계에서 반드시 고려해야 한다.

---

## 2.4 STUN / TURN

원문은 서로 다른 네트워크 환경에서 P2P가 성립하기 어려운 이유를 다음과 같이 설명한다.

- 같은 네트워크라면 **사설 IP로도 P2P 가능**
- 서로 다른 네트워크라면 **각자의 사설 IP로는 연결 불가능**
- 이때 STUN/TURN이 “NAT 뒤 장치들이 인터넷을 통해 서로 통신하도록” 도와준다

### 2.4.1 STUN(원문 요지)
원문은 STUN을 다음 비유로 설명한다.

- “나 지금 어디 있는 거야?”라고 물어볼 때 **공인 IP 정보를 알려주는 친절한 서버**

NAT 환경에서는 Private IP를 별도로 가지므로 외부에서 접근이 어렵다.  
따라서 클라이언트는 STUN 서버로 요청을 보내 **자신의 Public IP 정보를 확인**한다.

**특징(원문):** 가볍고 빠르게 주소 확인 가능.

### 2.4.2 TURN(원문 요지)
원문은 TURN을 다음 비유로 설명한다.

- “너희 둘이 만나기 어려우면, 내가 중간에서 메시지를 전해줄게!”라는 **중계자**

TURN은 STUN으로 해결할 수 없는 상황에서 사용된다.  
일부 NAT/방화벽은 외부 서버와의 직접 P2P 연결 자체를 허용하지 않기 때문이다.

**특징(원문):** 완벽한 중계 서비스를 제공하지만, 상대적으로 느리고 리소스를 많이 사용.

### 2.4.3 WebRTC의 기본 전략(원문 포함)
원문은 WebRTC가 다음 전략으로 동작한다고 정리한다.

1) “먼저 STUN으로 해볼까? 오 잘되네 이걸로 가자!”  
2) “STUN이 안 되네..? 그럼 TURN한테 부탁하자”

즉, **STUN → 실패 시 TURN** 순으로 fallback한다.

### 2.4.4 STUN/TURN 설정 코드(원문 포함)
원문에 제시된 `RTCPeerConnection` 설정 예시는 아래와 같다.

```js
const peerConnection = new RTCPeerConnection({
  iceServers: [
    { urls: 'stun:stun.l.google.com:19302' }, // STUN 서버
    {
      urls: 'turn:your-turn-server.com',      // TURN 서버
      username: '유저네임',
      credential: '비밀번호'
    }
  ]
});
```

> 확장(운영 관점):  
> - TURN은 “트래픽 중계”이므로 비용/대역폭/지연이 민감하다.  
> - STUN은 상대적으로 가볍지만, NAT 유형/방화벽 정책에 따라 성공률이 변한다.  
> - 서비스 품질을 위해서는 “TURN을 어느 정도 감당할 수 있는 설계”가 필요하다.

---

# 3. WebRTC 동작 원리(큰 그림 → 실제 교환 데이터)

원문은 “전체적인 큰 그림”을 먼저 제시하고, 이후 어떤 정보가 교환되는지 설명한다.

## 3.1 큰 그림(원문 요지)

서로 다른 네트워크 환경이라 가정할 때:

1. STUN 서버를 통해 연결 가능한 정보를 획득  
2. 시그널링 서버를 통해 SDP를 주고받음  
3. (중요) 이 SDP에는 ICE 후보(연결 가능한 IP 주소 정보)도 포함됨  
4. SDP 교환 및 WebRTC 연결 설정이 끝난 후에야 P2P 연결이 가능해짐

또한 원문은 **시그널링 서버의 수명**을 명확히 한다.

- 시그널링 서버는 **연결 과정에서만** 필요
- P2P 연결이 성립한 후에는 “더 이상 P2P 연결을 위한 시그널링 서버는 필요하지 않다”
- 이후 통신은 WebRTC를 통해 P2P로 이루어짐

## 3.2 흐름을 한 장으로 요약(확장)

아래는 원문 설명을 실제 구현 흐름으로 재정렬한 요약이다.

1) **클라이언트 A/B가 시그널링 채널(WebSocket/HTTP)에 접속**  
2) A가 **Offer SDP 생성** → 시그널링 서버를 통해 B에게 전달  
3) B가 Offer를 setRemoteDescription → **Answer SDP 생성** → A에게 전달  
4) A/B가 각각 **ICE 후보 수집**(host/srflx/relay 등)  
5) ICE 후보를 시그널링으로 교환하며 connectivity check  
6) 최적 후보가 선택되면 **P2P 경로 확정**  
7) 이후 RTP/SRTP 기반으로 미디어가 전달(직접 또는 TURN relay)

---

# 4. P2P(Mesh) 연결의 한계와 규모 확장 방식

원문은 “WebRTC 개발을 위해 꼭 알아야 하는 부분”으로 **P2P 연결의 한계**를 제시한다.

## 4.1 왜 한계가 생기는가(원문 요지)

- P2P는 WebRTC의 기본이지만, 참가자가 많아지면 과부하가 오기 시작한다.
- N명이 참여하면 필요한 연결 수는 `N(N-1)/2`  
  - 예: 10명 참여 시 45개 연결
- 결과적으로 CPU(및 네트워크) 과부하 가능성이 매우 높아진다.

하지만 원문은 동시에 P2P의 장점도 함께 명시한다.

- 서버에 별도 부하가 거의 없음
- Peer 간 직접 연결로 실시간성이 보장(낮은 지연)
- 따라서 **매우 적은 사용자**를 대상으로 할 때 유용

## 4.2 현실적인 결론(원문 요지)
원문은 “그럼 어떻게 효율적으로 처리할 수 있을까?”라는 질문을 던지고,  
해결 방식으로 SFU/MCU를 간단히 짚고 마무리한다.

---

# 5. SFU vs MCU 상세 비교 + 적용 사례

원문은 P2P 한계를 해결하는 대표 방식으로 **SFU, MCU**를 소개한다.

---

## 5.1 SFU(Selective Forwarding Unit)

### 5.1.1 정의(원문 요지)
SFU는 “영리한 미디어 라우터”로 설명된다.  
중앙 서버가 미디어 트래픽을 중계하며 서버-클라이언트 간 peer를 연결한다.

### 5.1.2 특징(원문 요지)
- 각 참가자로부터 **하나의 스트림만** 받아 필요한 곳에 전달  
- 참가자의 대역폭/성능에 따라 영상 품질 조절 가능  
- 확장성이 뛰어나고 지연이 비교적 짧음

### 5.1.3 작동 방식(원문 포함)
1. A가 영상을 SFU로 **한 번만** 전송  
2. SFU가 영상을 B/C/D에게 각각 전달  
3. A는 업로드를 한 번만 하면 됨  
→ 1:N 또는 소규모 N:M 스트리밍에 주로 채택

### 5.1.4 확장(실무 관점)
SFU는 클라이언트 단에서 “여러 스트림을 다운받는 부담”이 있다.  
하지만 업로드는 1회라서 P2P Mesh보다 훨씬 안정적이고, 서버는 “전달(Forwarding)” 중심이라 MCU보다 상대적으로 가볍다.

---

## 5.2 MCU(Multipoint Control Unit)

### 5.2.1 정의(원문 요지)
MCU는 “영상을 받아서 섞은 다음 하나의 스트림으로 만들어 보낸다”고 설명된다.

### 5.2.2 특징(원문 요지)
- 클라이언트 부담이 가장 적고 일관된 경험 제공  
- 서버 리소스를 가장 많이 사용  
- 최대 단점: WebRTC의 장점인 **실시간성이 보장되지 않을 수 있음**(인코딩/믹싱 오버헤드로 지연 가능)

### 5.2.3 작동 방식(원문 포함)
1. A/B/C가 각자의 영상을 MCU로 전송  
2. MCU가 영상들을 **하나의 스트림으로 합성**  
3. 통합 스트림을 모든 참가자에게 전송

---

## 5.3 SFU vs MCU 요약(원문 포함)

- SFU: 유연하고 확장성이 좋음. **대규모 회의에 적합**
- MCU: 일관된 경험 + 클라이언트 부담이 적음. **작은 회의나 낮은 성능 기기에 유리**

### 5.3.1 적용 사례(원문 포함)
원문은 알려진 서비스 예시로 다음을 언급한다.

- Zoom: SFU와 MCU의 하이브리드 방식 사용  
- Google Meet: SFU 방식 사용

> 확장(주의):  
> 이 부분은 원문에서 “~라고 하네요” 수준의 참고 서술이다.  
> 실제 구현은 서비스 버전/토폴로지/기능(예: 배경 블러, 레이아웃 합성 등)에 따라 복합적이다.  
> 다만 학습 목적의 “경향성”으로 이해하면 유용하다.

---

# 6. openVidu v3 개요: 왜 선택하는가

두 번째 글은 “프로젝트에서 WebRTC 기반 원격 방(음성+화상)을 구현해야 하는 상황”을 전제로 한다.

## 6.1 문제 의식(원문 요지)

- WebRTC를 실적용하기 위한 서버/플랫폼 선택지가 많다  
- 기한 내 구현을 위해 “처음부터 low-level로 구축”은 부담이 크다  
- 그래서 WebRTC 기반 실시간 화상통화를 도와주는 플랫폼 **openVidu**를 선택하게 됨

즉, openVidu는 “WebRTC를 직접 다루는 복잡도”를 줄여, 비교적 빠르게 기능 구현을 가능하게 하는 접근이다.

## 6.2 v3 등장(원문 요지)

- 2024년 6월 openVidu v3 출시  
- 글 작성 시점(2024년 7월 말) 기준 출시 약 1달 → 자료가 매우 적음  
- 따라서 공식 문서 기반으로 v3 내용을 간단히 정리하고  
  Java(SpringBoot)+React 튜토리얼을 직접 실행해보는 것이 글의 목적

## 6.3 (가볍게) WebRTC 정의 재확인(원문 요지)

openVidu 글에서도 WebRTC를 짧게 정의한다.

- 브라우저/기기 간 실시간 음성/텍스트/화상 통신을 가능하게 하는 오픈소스  
- 브라우저를 통한 P2P 통신이므로 플러그인/앱 설치 없이 가능  
- 하지만 실제로 안정적으로 사용하려면 노력과 지식이 많이 필요하며  
  어중간한 구현은 “끊김 등 불안정”으로 이어질 수 있음

이 문맥이 openVidu를 선택하는 “현실적 이유”로 연결된다.

---

# 7. openVidu v2 vs v3 변화점(핵심 2가지) + 미지원 기능

원문은 v3가 “요즘 대세를 따르기 위해 새롭게 개편된 플랫폼”이라는 관점을 제시하며, 핵심 변화점 2가지를 정리한다.

## 7.1 변화점 1) SFU 서버 변경: Kurento → mediasoup

- v2는 **Kurento 기반**  
  - openVidu 개발자들이 제작한 SFU 위에 openVidu 플랫폼을 얹어 사용성을 높인 구조  
- v3에서는 Kurento가 낡고 성능 한계가 명확해졌다고 판단하여  
  내부 서버를 **mediasoup로 변경**했다고 한다.

> 확장(의미):  
> SFU 엔진 교체는 성능/확장성/유지보수성에 직접 영향을 준다.  
> mediasoup는 현대 WebRTC SFU 스택에서 자주 언급되는 선택지이며, LiveKit 등과도 궁합이 좋아 “생태계” 측면의 장점이 있을 수 있다.

## 7.2 변화점 2) LiveKit 적용

- v2는 LiveKit 미적용  
- v3는 LiveKit 적용  
- 그 결과, “LiveKit 기반으로 설계된 서버/클라이언트라면 그대로 사용할 수 있다(는 것 같다)”고 원문은 설명한다.  
- LiveKit은 openAI에서도 사용하고 있는 오픈소스라는 언급도 포함된다.

> 확장(의미):  
> LiveKit 기반으로 가면 “클라이언트 SDK/서버 SDK/토큰/룸 운영 방식”이 LiveKit의 표준 패턴으로 정착된다.  
> 즉 openVidu v3를 쓰는 팀은 사실상 LiveKit 생태계를 함께 가져오는 셈이다.

## 7.3 (주의) 미지원 기능(원문 포함)

2024년 7월 말 기준, 다음 기능은 아직 지원하지 않는다고 원문은 명시한다.

- IP Cameras  
- Speech To Text

---

# 8. openVidu v3 아키텍처(3요소)와 역할

원문은 openVidu v3 기반 애플리케이션이 **3가지**로 나뉜다고 정리한다.

1) openVidu deployment  
2) Application client  
3) Application server

---

## 8.1 openVidu deployment (Infrastructure)

### 8.1.1 무엇인가(원문 요지)
실시간 화상/음성 통화를 위해 필요한 모든 인프라를 openVidu가 제공하며,  
LiveKit 서버와 mediasoup 서버 기반이라고 설명한다.

원문 표현: 사용자는 “이 deployment에 무엇이 들었는지 알 필요가 없다”  
- 원문에서 *“be treated as a black box”*라는 표현이 쓰였다고 언급

### 8.1.2 하지만 완전히 무시할 수는 없다(원문 요지)
- docker 기반으로 deployment를 사용할 때  
  - 포트를 많이 사용하고  
  - 컨테이너가 대략 9개 정도 올라가며  
  - “포트를 먹어치우는 돼지”라고 표현될 정도로 리소스를 사용한다  
- 실제 배포에서는 **포트 충돌**에 주의해야 한다고 코멘트한다.

> 확장(실무 관점):  
> 로컬 환경(특히 팀 프로젝트)에서 포트 충돌은 빈번하다.  
> - 이미 사용 중인 80/443/3000/8080/5443 등  
> - 프록시/게이트웨이/관측 도구(예: Prometheus)  
> 와 겹치면 구동 자체가 막힌다.  
> 따라서 “포트 설계 문서”를 먼저 잡는 것이 안전하다.

---

## 8.2 Application Client (Frontend)

### 8.2.1 역할(원문 요지)
- 흔히 말하는 프론트엔드  
- LiveKit client SDK 기반으로 server 및 deployment와 통신  
- 방 입장을 위해서는 **server에서 발급한 토큰**이 필요

> 확장(의미):  
> 토큰은 “누가, 어떤 룸에, 어떤 권한으로 들어오는지”를 서버가 통제하기 위한 핵심 장치다.  
> 인증/인가/요금제/호스트 권한 등 거의 모든 정책이 토큰 발급 로직에 결합된다.

---

## 8.3 Application Server (Backend)

### 8.3.1 역할(원문 요지)
- 흔히 말하는 백엔드  
- LiveKit server SDK 기반으로 deployment와 통신  
- openVidu v3로 화상 통화를 구현하려면 최소한 다음 2가지를 제공해야 한다.
  1) 토큰 발급  
  2) 룸 참여

> 확장(실무 관점):  
> 실제 서비스에서는 여기에 보통 다음이 추가된다.  
> - 룸 메타데이터 관리(제목/최대 인원/상태)  
> - 참여자 관리(강퇴/차단/호스트 이관)  
> - 이벤트 처리(webhook 수신 후 DB 반영)  
> - 녹화/아카이빙/하이라이트 추출 연동  
> - 장애 대응(재접속 정책, TURN 비용 모니터링)

---

# 9. 튜토리얼 실습: Deployment → Server → Client

원문은 “백문이 불여일견”이라는 문장으로 실습을 시작하며,  
3요소를 차근차근 구축해보자고 안내한다.

---

## 9.1 Deployment 가져오기(원문 단계 포함)

### 9.1.1 전제 조건
- openVidu v3의 핵심인 deployment가 떠 있어야 프로그램이 동작  
- local deployment 방식으로 간단 테스트를 할 예정  
- 로컬 PC에 **Docker 설치가 필수**  
- 원문은 **Windows 11 기준**으로 설명한다고 명시

### 9.1.2 Community vs Pro(원문 요지)
- 튜토리얼 원문에는 community/pro 버전이 분리되어 설명됨  
- 글에서는 **community 기준**으로 설명  
- 이유: pro는 유료 버전이기 때문  
- 다만 글 작성 시점에는 v3가 beta라 pro도 무료 사용 가능하지만,  
  “언젠가는 유료가 될 것”이라는 현실적 코멘트를 덧붙임

### 9.1.3 local deployment 레포 클론(원문 포함)
```bash
git clone https://github.com/OpenVidu/openvidu-local-deployment
```

### 9.1.4 LAN_PRIVATE_IP 설정(원문 포함)
클론한 폴더에서 `community` 폴더로 이동하고 Windows용 `.bat`를 실행한다.  
이 배치 파일은 `.env` 파일 내부의 `LAN_PRIVATE_IP`(현재 기기의 private ip)를 설정하는 작업을 수행한다.

```bash
cd openvidu-local-deployment/community
.\configure_lan_private_ip_windows.bat
```

### 9.1.5 docker compose 실행(원문 포함)
compose 파일 기반으로 실행한다. 백그라운드 실행을 원하면 `-d` 옵션을 붙인다.

```bash
docker compose up
# (선택) docker compose up -d
```

정상 실행이 끝나면 “설정 다 됐으니 이용하라”는 메시지가 보일 것이라고 원문은 안내한다.

---

## 9.2 Application Server 가져오기(원문 단계 포함)

### 9.2.1 튜토리얼 서버 레포 클론(원문 포함)
```bash
git clone https://github.com/OpenVidu/openvidu-livekit-tutorials.git
```

### 9.2.2 SpringBoot 서버 실행(원문 요지)
원문은 `openvidu-livekit-tutorials/application-server/java` 경로의 파일을 기반으로 SpringBoot 프로젝트를 실행하면 된다고 설명한다.

- 공식 문서에서는 `mvn` 명령으로 실행  
- 글 작성자는 IntelliJ에서 프로젝트를 열어 실행했다고 언급

### 9.2.3 서버 코드 구성(원문 요지)
프로젝트 내부는 단순하고 Controller 중심이며, 원문은 다음 2개 엔드포인트 성격을 설명한다.

- `createToken`: 룸 입장을 위한 토큰 발급 API  
- `webhook` 수신: 화상 방에서 발생하는 이벤트를 알려주는 통신(webhook)을 받는 부분

> 확장(의미):  
> webhook 이벤트는 보통  
> - 사용자 join/leave  
> - 트랙 publish/unpublish  
> - 룸 생성/종료  
> 같은 사실을 서버에 통보한다.  
> 이를 이용하면 “참여 로그”, “과금/정산”, “비정상 종료 감지” 등을 구현할 수 있다.

---

## 9.3 Application Client 가져오기(원문 단계 포함)

### 9.3.1 React 클라이언트 위치(원문 요지)
위에서 clone한 레포 내에서 다음 경로로 이동하면 React 프로젝트가 구성되어 있다고 설명한다.

- `openvidu-livekit-tutorials/application-client/openvidu-react`

### 9.3.2 실행(원문 포함)
필요 의존성을 설치하고 실행하면 된다고 안내한다.

```bash
npm install
npm run
```

> 주의(확장):  
> 일반적으로 React는 `npm run dev` 또는 `npm start`가 흔하지만,  
> 튜토리얼 스크립트 구성에 따라 `npm run`이 특정 스크립트를 가리킬 수 있다.  
> 실제로는 `package.json`의 scripts를 확인하는 것이 안전하다.

### 9.3.3 코드 구성(원문 요지)
원문은 “서버는 간단하지만 클라이언트는 조금 복잡”하다고 말하며, 다음 구성요소를 간단히 소개한다.

- `AudioComponent.tsx`, `VideoComponent.tsx`: 오디오/비디오 송출 관련  
- `App.tsx`: openVidu v3 메인 메커니즘 관리 + 화면 구성

---

# 10. 실습 후 체크포인트/트러블슈팅(카메라 X 포함)

## 10.1 접속 테스트(원문 요지)

클라이언트를 실행해 페이지에 들어가면:

- 참가자 이름 입력 칸
- 방 이름 입력 칸

이때 방 이름은 단순 라벨이 아니라, openVidu v3에서 “각 방을 구분하는 고유 값”으로 사용된다고 원문은 설명한다.

테스트 방법(원문):
- 브라우저 탭 2개를 띄우고  
- 같은 방 이름으로 접속  
→ 정상적으로 같은 방에서 통신되는 것을 확인 가능

원문은 이를 보고 “다운로드하고 실행만 했는데 화상통화가 바로 구현됐다”고 감탄(“openVidu는 신인가?”)하는 표현을 남긴다.

---

## 10.2 “만약 카메라가 안 된다면”(원문 전체 포함)

원문은 튜토리얼대로 했는데 카메라가 정상 송출되지 않고, 화면에 커다란 “X”가 뜨는 케이스를 설명한다.

- 글 작성자도 동일 문제를 겪었고  
- 호환성/프로그램 오류로 겁먹었지만  
- 다행히 큰 문제가 아니었다고 결론

원인(원문):
- 튜토리얼 프로그램이 사용할 카메라를 자동으로 가져오는데  
- 작성자 환경에서는 실제 카메라가 아니라  
  `mirametrix virtual camera`라는 “가상 카메라”를 잡고 있었음

해결(원문):
- 해당 가상 카메라를 비활성화하니 정상 출력

즉, 같은 증상이면 “가상 카메라/디바이스 선택”을 확인하라는 팁이다.

---

# 11. 마무리: 학습/확장 로드맵

## 11.1 WebRTC 정리(원문 결론 요지)

원문은 WebRTC 글에서 다음을 학습했다고 정리한다.

- WebRTC가 무엇인지  
- 실시간 통신이 어떻게 이루어지는지  
- P2P의 한계  
- 상황에 따라 선택할 수 있는 방식(P2P/SFU/MCU)

그리고 P2P/SFU/MCU는 “각각 상황에 맞게 사용하면 되고 정답은 없다”는 메시지로 마무리한다.  
또한 다음 포스팅에서 실제 코드 예제로 P2P 화상채팅을 다룰 예정이라고 예고한다.

## 11.2 openVidu v3 정리(원문 결론 요지)

openVidu v3 글은 다음으로 마무리한다.

- v3 개요 + 3가지 구성요소로 실제 화상통화 기능을 사용해봄  
- 작성해야 하는 코드는 대부분 client 쪽에 몰려있어 더 공부가 필요  
- 여유가 되면 client에서 openVidu를 이용하는 법을 더 파는 포스팅을 써보고 싶다

## 11.3 다음 학습 제안(확장)

원문 흐름을 기반으로, 다음 단계로 이어지는 학습을 추천한다.

1) **WebRTC 디버깅 역량**  
   - ICE state, candidate 타입(host/srflx/relay) 이해  
   - 실패 환경 분류(회사망/모바일/공항 와이파이/VPN)  
   - TURN 비용/성능 계측

2) **SFU 기반 운영 이해**  
   - “왜 P2P는 소규모에만 적합한가”를 수치로 설명할 수 있어야 함  
   - SFU에서 클라이언트 부담(다운 스트림 수 증가)과 네트워크 품질 적응

3) **openVidu v3/LiveKit 생태계**  
   - 토큰 발급(권한/수명/역할) 설계  
   - webhook 이벤트를 DB/로그/모니터링으로 연결  
   - 배포 시 포트/컨테이너/자원량 계획(원문에서 언급한 ‘포트 충돌’ 포함)

---

## 참고 문헌(원문에 포함된 링크 목록 재기재)

아래는 WebRTC 원문 말미에 제공된 참고 문헌 링크들이다(원문 그대로 목록화).

- https://developer.mozilla.org/ko/docs/Glossary/ICE  
- https://webrtc.org/?hl=ko  
- https://youtu.be/bWcNEk0H4Y0  
- https://youtu.be/8I2axE6j204  
- https://bloggeek.me/webrtc-p2p-mesh/  
- https://webrtc.ventures/2021/08/liveswitch-telehealth-video-application/  
- https://millo-l.github.io/WebRTC-%EA%B5%AC%ED%98%84-%EB%B0%A9%EC%8B%9D-Mesh-SFU-MCU/

---

### 문서 버전
- v1.0 (2026-01-16): 원문 2개 글 기반 “전체 내용 포함 + 상세 확장” 정리본

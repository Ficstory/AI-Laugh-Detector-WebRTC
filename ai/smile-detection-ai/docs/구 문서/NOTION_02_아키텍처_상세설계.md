# ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„¸ ì„¤ê³„

ì‘ì„±ì¼: 2026-01-12
ë²„ì „: v1.0

---

# ğŸ›ï¸ ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

## ì‹œìŠ¤í…œ í”Œë¡œìš°

**í´ë¼ì´ì–¸íŠ¸ (í”„ë¡ íŠ¸ì—”ë“œ)**

WebRTC Video Stream â†’ Frame Extractor (30 FPS) â†’ Base64 Image

â†“ HTTP POST

**FastAPI ì„œë²„ (ë°±ì—”ë“œ)**

ìš”ì²­ í•¸ë“¤ëŸ¬ & í”„ë ˆì„ ë²„í¼
- User Buffer 1: [10 frames]
- User Buffer 2: [10 frames]

â†“

**AI ëª¨ë¸ (PyTorch)**

INPUT: [Batch, 10, 3, 224, 224]

â†“

CNN Feature Extractor (MobileNetV3-Small, Pretrained on ImageNet)

â†“ [Batch, 10, 256]

Bidirectional LSTM
- Hidden: 128
- Layers: 2
- Dropout: 0.2

â†“ [Batch, 256] (last hidden state)

Classifier Head
- FC(256â†’64) + ReLU + Dropout(0.5)
- FC(64â†’2)
- Softmax

â†“ [Batch, 2] [No Smile, Smile]

**ì¶œë ¥**: {"is_smiling": true, "confidence": 0.87}

---

# ğŸ” ê° ì»´í¬ë„ŒíŠ¸ ìƒì„¸ ì„¤ê³„

## 1. ì…ë ¥ ì „ì²˜ë¦¬

### ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

**ì²˜ë¦¬ ë‹¨ê³„**

**Step 1**: Resize to 224x224
- ì…ë ¥: ê°€ë³€ í¬ê¸° (720p/1080p)
- ì¶œë ¥: 224x224

**Step 2**: RGB ë³€í™˜
- OpenCVëŠ” BGR ì‚¬ìš©
- RGBë¡œ ë³€í™˜ í•„ìš”

**Step 3**: Normalize [0, 255] â†’ [0, 1]
- í”½ì…€ ê°’ì„ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”

**Step 4**: ImageNet í‘œì¤€í™”
- mean = [0.485, 0.456, 0.406]
- std = [0.229, 0.224, 0.225]

**Step 5**: HWC â†’ CHW
- Height, Width, Channel â†’ Channel, Height, Width
- PyTorch í¬ë§·

### ì‹œí€€ìŠ¤ êµ¬ì„±

10ê°œ í”„ë ˆì„ì„ ìŠ¤íƒ:
- ê° í”„ë ˆì„: (3, 224, 224)
- ì‹œí€€ìŠ¤: (10, 3, 224, 224)
- ë°°ì¹˜ ì°¨ì› ì¶”ê°€: (1, 10, 3, 224, 224)

---

## 2. CNN Feature Extractor

### MobileNetV3-Small êµ¬ì¡°

**ê¸°ë³¸ ì •ë³´**
- íŒŒë¼ë¯¸í„° ìˆ˜: 2.5M
- ì…ë ¥: (224, 224, 3)
- ì›ë³¸ ì¶œë ¥: 1024-dim
- ìˆ˜ì • ì¶œë ¥: 256-dim

### ë ˆì´ì–´ êµ¬ì¡°

**Stem**
- Conv 3x3, stride=2
- (3, 224, 224) â†’ (16, 112, 112)

**Block 1**: MBConv, expansion=1
- (16, 112, 112) â†’ (16, 56, 56)

**Block 2**: MBConv, expansion=4
- (16, 56, 56) â†’ (24, 28, 28)

**Block 3**: MBConv, expansion=3
- (24, 28, 28) â†’ (40, 14, 14)

**Block 4**: MBConv, expansion=6
- (40, 14, 14) â†’ (48, 14, 14)

**Block 5**: MBConv, expansion=6
- (48, 14, 14) â†’ (96, 7, 7)

**Head**: Conv 1x1
- (96, 7, 7) â†’ (576, 7, 7)

**Global Average Pooling**
- (576, 7, 7) â†’ (576,)

**Custom Layer**: Linear(576 â†’ 256) + ReLU
- (576,) â†’ (256,)

---

### ìš°ë¦¬ì˜ ìˆ˜ì •ì‚¬í•­

Pretrained MobileNetV3 ë¡œë“œ í›„:

**ë§ˆì§€ë§‰ ë¶„ë¥˜ ë ˆì´ì–´ êµì²´**
- ê¸°ì¡´: Linear(576 â†’ 1000) for ImageNet
- ìˆ˜ì •: Linear(576 â†’ 256) + Hardswish + Dropout(0.3)

ëª©ì : 256ì°¨ì› feature vector ì¶”ì¶œ

---

### ì£¼ìš” íŠ¹ì§•

**1. Squeeze-and-Excitation (SE) ë¸”ë¡**
- ì±„ë„ ê°„ ì¤‘ìš”ë„ í•™ìŠµ
- ì¤‘ìš”í•œ íŠ¹ì§• ê°•ì¡°

**2. H-Swish í™œì„±í™” í•¨ìˆ˜**
- ReLUë³´ë‹¤ ë¶€ë“œëŸ½ê³  íš¨ìœ¨ì 
- ëª¨ë°”ì¼ í™˜ê²½ ìµœì í™”

**3. Depthwise Separable Convolution**
- ì¼ë°˜ Convolutionë³´ë‹¤ ì—°ì‚°ëŸ‰ ê°ì†Œ
- ì„±ëŠ¥ì€ ìœ ì§€

---

### Fine-tuning ì „ëµ

**Phase 1: Freeze Backbone**

ëª©ì : Pretrained íŠ¹ì§• ë³´ì¡´

ì„¤ì •:
- Backbone (features) freeze
- Classifierë§Œ í•™ìŠµ
- Learning rate: 1e-3
- Epochs: 10

**Phase 2: Unfreeze All**

ëª©ì : End-to-end í•™ìŠµ

ì„¤ì •:
- ì „ì²´ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ
- Learning rate: 1e-4 (10ë°° ê°ì†Œ)
- Epochs: 30-40

---

## 3. LSTM Temporal Module

### êµ¬ì¡° ìƒì„¸

**ì„¤ì •**
- input_size: 256 (CNN feature dim)
- hidden_size: 128 (LSTM hidden dim)
- num_layers: 2 (Stack 2 LSTM layers)
- batch_first: True (Input: Batch, Seq, Feature)
- dropout: 0.2 (Dropout between LSTM layers)
- bidirectional: True (Forward + Backward)

### ì…ì¶œë ¥

**ì…ë ¥**: (Batch, Sequence=10, Features=256)

**ì¶œë ¥**:
- output: (Batch, Seq, HiddenÃ—2=256)
- h_n: (num_layersÃ—2, Batch, Hidden=128)
- c_n: (num_layersÃ—2, Batch, Hidden=128)

**ìµœì¢… ì‚¬ìš©**: output[:, -1, :] â†’ (Batch, 256)
- ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì¶œë ¥ë§Œ ì‚¬ìš©

---

### Bidirectional LSTM ë™ì‘ ì›ë¦¬

**ì‹œí€€ìŠ¤**: [f1, f2, f3, ..., f10]

**Forward LSTM**
f1 â†’ f2 â†’ f3 â†’ ... â†’ f10 â†’ h_forward (128-dim)

**Backward LSTM**
f10 â†’ f9 â†’ f8 â†’ ... â†’ f1 â†’ h_backward (128-dim)

**ìµœì¢… ì¶œë ¥**
[h_forward; h_backward] (concatenate) â†’ 256-dim

---

### ì¥ì 

**ë¯¸ë˜ì™€ ê³¼ê±° ì •ë³´ ëª¨ë‘ í™œìš©**
- Forward: ì›ƒìŒ ì‹œì‘ ê°ì§€
- Backward: ì›ƒìŒ ë í™•ì¸
- ì–‘ë°©í–¥ ì •ë³´ë¡œ ì •í™•ë„ í–¥ìƒ (ì˜ˆìƒ +3~5%)

**ì›ƒìŒ ë§¥ë½ íŒŒì•…**
- ì›ƒìŒ ì „: ì¤‘ë¦½ í‘œì •
- ì›ƒìŒ ì¤‘: ì… ëª¨ì–‘ ë³€í™”
- ì›ƒìŒ í›„: ì”ì—¬ ë¯¸ì†Œ

---

### Hidden State ì´ˆê¸°í™”

**ê¸°ë³¸ ë°©ì‹**: Zero initialization (ìë™)
- h_0 = zeros(num_layersÃ—2, batch, hidden_size)
- c_0 = zeros(num_layersÃ—2, batch, hidden_size)

**í•™ìŠµ ê°€ëŠ¥í•œ ì´ˆê¸°í™”** (ì˜µì…˜):
- íŒŒë¼ë¯¸í„°ë¡œ ë“±ë¡í•˜ì—¬ í•™ìŠµ
- ë” ë‚˜ì€ ì´ˆê¸° ìƒíƒœ í•™ìŠµ ê°€ëŠ¥

---

## 4. Classifier Head

### êµ¬ì¡°

**Layer 1**
- Linear(256 â†’ 64)
- ReLU í™œì„±í™”
- Dropout(0.5)

**Layer 2 (Output)**
- Linear(64 â†’ 2)
- ì¶œë ¥: [No Smile, Smile] logits

---

### Softmax ë° ì¶œë ¥

**í•™ìŠµ ì‹œ**
- logits ì¶œë ¥
- CrossEntropyLoss ê³„ì‚° (ë‚´ë¶€ì— Softmax í¬í•¨)

**ì¶”ë¡  ì‹œ**
- logits â†’ Softmax â†’ í™•ë¥ 
- ì˜ˆ: [0.13, 0.87] â†’ Smile í™•ë¥  87%

---

## 5. ì „ì²´ ëª¨ë¸ í†µí•©

### Forward Pass íë¦„

**ì…ë ¥**: (Batch=4, Sequence=10, Channels=3, H=224, W=224)

**Step 1**: CNN ì²˜ë¦¬ë¥¼ ìœ„í•œ Reshape
- (4, 10, 3, 224, 224) â†’ (40, 3, 224, 224)
- Batchì™€ Sequence ì°¨ì› ë³‘í•©

**Step 2**: CNN Forward
- (40, 3, 224, 224) â†’ (40, 256)
- ê° í”„ë ˆì„ ë…ë¦½ì ìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œ

**Step 3**: LSTM ì²˜ë¦¬ë¥¼ ìœ„í•œ Reshape
- (40, 256) â†’ (4, 10, 256)
- Batchì™€ Sequence ì°¨ì› ë³µì›

**Step 4**: LSTM Forward
- (4, 10, 256) â†’ (4, 10, 256)
- ì‹œê°„ì  íŒ¨í„´ í•™ìŠµ

**Step 5**: ë§ˆì§€ë§‰ Hidden State ì¶”ì¶œ
- (4, 10, 256) â†’ (4, 256)
- [:, -1, :] ì¸ë±ì‹±

**Step 6**: Classifier Forward
- (4, 256) â†’ (4, 2)
- ìµœì¢… logits ì¶œë ¥

**Step 7**: Softmax
- (4, 2) â†’ í™•ë¥ 
- [[0.92, 0.08], [0.15, 0.85], [0.78, 0.22], [0.05, 0.95]]

---

# ğŸ“Š ëª¨ë¸ ë³µì¡ë„ ë¶„ì„

## íŒŒë¼ë¯¸í„° ìˆ˜

| ëª¨ë“ˆ | íŒŒë¼ë¯¸í„° ìˆ˜ | ë¹„ìœ¨ |
|------|-----------|------|
| MobileNetV3 Backbone | 2,500,000 | 84% |
| MobileNetV3 Classifier (ìˆ˜ì •) | 150,000 | 5% |
| LSTM (2 layers, bidirectional) | 280,000 | 9% |
| Classifier Head | 50,000 | 2% |
| **Total** | **~3,000,000** | **100%** |

---

## ì—°ì‚°ëŸ‰ (FLOPs)

### ë‹¨ì¼ ì¶”ë¡ ë‹¹ ê³„ì‚°ëŸ‰

**MobileNetV3 (224x224 ì…ë ¥)**: ~60M FLOPs
- 10 í”„ë ˆì„: 600M FLOPs

**LSTM (10 timesteps, 256â†’128)**: ~15M FLOPs

**Classifier**: ~0.5M FLOPs

**Total**: ~615M FLOPs per inference

### ë¹„êµ (ImageNet ë¶„ë¥˜)

| ëª¨ë¸ | FLOPs |
|------|-------|
| ResNet50 | ~4,100M |
| EfficientNet-B0 | ~400M |
| **ìš°ë¦¬ ëª¨ë¸** | **~615M** |

ê²°ë¡ : í—ˆìš© ë²”ìœ„ ë‚´

---

## ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

### ëª¨ë¸ ê°€ì¤‘ì¹˜

| ì •ë°€ë„ | í¬ê¸° |
|--------|------|
| FP32 | 3M Ã— 4 bytes = 12 MB |
| FP16 | 3M Ã— 2 bytes = 6 MB |
| INT8 | 3M Ã— 1 byte = 3 MB |

### ì¶”ë¡  ì‹œ ë©”ëª¨ë¦¬ (Batch=1)

| í•­ëª© | í¬ê¸° |
|------|------|
| ì…ë ¥ í…ì„œ | 10Ã—3Ã—224Ã—224Ã—4 bytes = 6 MB |
| ì¤‘ê°„ í™œì„±í™” | ~20 MB |
| ì¶œë ¥ | < 1 MB |
| **Total** | **~30 MB per user** |

### ë‹¤ì¤‘ ì‚¬ìš©ì

- 10ëª… ë™ì‹œ ì ‘ì†: ~300 MB
- 100ëª…: ~3 GB (ë©”ëª¨ë¦¬ ê´€ë¦¬ í•„ìš”)

---

# âš¡ ì¶”ë¡  ì†ë„ ìµœì í™”

## 1. TorchScript ë³€í™˜

**ë°©ë²•**: ëª¨ë¸ì„ TorchScriptë¡œ ë³€í™˜

ê³¼ì •:
- model.eval() ëª¨ë“œ ì„¤ì •
- example_input ì¤€ë¹„
- torch.jit.trace() ì‹¤í–‰
- traced_model.save() ì €ì¥

**íš¨ê³¼**:
- ì†ë„ í–¥ìƒ: 10-30%
- Python ì˜¤ë²„í—¤ë“œ ì œê±°
- C++ ëŸ°íƒ€ì„ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥

---

## 2. Mixed Precision (FP16)

**ë°©ë²•**: AMP (Automatic Mixed Precision)

**íš¨ê³¼**:
- ë©”ëª¨ë¦¬ ì ˆì•½: 50%
- ì†ë„ í–¥ìƒ: 20-40% (Tensor Cores ì‚¬ìš© ì‹œ)

**ì ìš© ëŒ€ìƒ**:
- GPU with Tensor Cores (V100, A100, RTX ì‹œë¦¬ì¦ˆ)

---

## 3. ë°°ì¹˜ ì²˜ë¦¬

**ì „ëµ**: ì—¬ëŸ¬ ì‚¬ìš©ì ìš”ì²­ì„ ë°°ì¹˜ë¡œ ë¬¶ê¸°

ì˜ˆì‹œ:
- user1_sequence: (10, 3, 224, 224)
- user2_sequence: (10, 3, 224, 224)
- user3_sequence: (10, 3, 224, 224)

ë¬¶ìŒ:
- batch: (3, 10, 3, 224, 224)
- outputs: (3, 2)

**íš¨ê³¼**:
- GPU í™œìš©ë¥  ì¦ê°€
- ì „ì²´ ì²˜ë¦¬ëŸ‰ (Throughput) ì¦ê°€

---

## 4. ONNX Runtime (ì˜µì…˜)

**ë°©ë²•**: PyTorch â†’ ONNX ë³€í™˜

**íš¨ê³¼**:
- ì†ë„ í–¥ìƒ: 20-50%
- CPUì—ì„œ íŠ¹íˆ íš¨ê³¼ì 
- ë‹¤ì–‘í•œ í”Œë«í¼ ì§€ì›

**ê³ ë ¤ì‚¬í•­**:
- ë³€í™˜ ë³µì¡ë„
- ë””ë²„ê¹… ì–´ë ¤ì›€
- í•„ìš” ì‹œ ì ìš©

---

# ğŸ“ í•™ìŠµ ì „ëµ

## ì†ì‹¤ í•¨ìˆ˜

### ê¸°ë³¸: CrossEntropyLoss

ì¼ë°˜ ì‚¬ìš©:
- ì…ë ¥: logits (Batch, 2)
- íƒ€ê²Ÿ: labels (Batch,)
- ì¶œë ¥: loss

### í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬

**ë¬¸ì œ**: ë¹„ì›ƒìŒ 70%, ì›ƒìŒ 30%

**í•´ê²°**: ê°€ì¤‘ì¹˜ ë¶€ì—¬
- ë¹„ì›ƒìŒ: 1.0 (ë‹¤ìˆ˜ í´ë˜ìŠ¤)
- ì›ƒìŒ: 2.3 (ì†Œìˆ˜ í´ë˜ìŠ¤, 300/130)

**íš¨ê³¼**: ì›ƒìŒ í´ë˜ìŠ¤ í•™ìŠµ ê°•í™”

---

## Optimizer: Adam

**ì„¤ì •**:
- learning_rate: 1e-3
- weight_decay: 1e-4 (L2 regularization)

**ì„ íƒ ì´ìœ **:
- ì ì‘í˜• í•™ìŠµë¥ 
- ë¹ ë¥¸ ìˆ˜ë ´
- ì•ˆì •ì 

---

## Learning Rate Scheduler: Cosine Annealing

**ì„¤ì •**:
- T_max: 50 epochs
- eta_min: 1e-6

**ë™ì‘**:
- ì´ˆê¸°: 1e-3
- ì¤‘ê°„: ì ì§„ì  ê°ì†Œ
- ë: 1e-6

**ì¥ì **:
- ë¶€ë“œëŸ¬ìš´ í•™ìŠµë¥  ê°ì†Œ
- ìˆ˜ë ´ ì•ˆì •ì„±
- Local minima íƒˆì¶œ ê°€ëŠ¥

---

## Early Stopping

**ì„¤ì •**:
- patience: 10 epochs
- min_delta: 0.001

**ë™ì‘**:
- Validation loss ê°œì„  ì—†ìœ¼ë©´ ì¹´ìš´í„° ì¦ê°€
- patience ë„ë‹¬ ì‹œ í•™ìŠµ ì¤‘ë‹¨
- ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥

**ëª©ì **:
- ê³¼ì í•© ë°©ì§€
- í•™ìŠµ ì‹œê°„ ì ˆì•½

---

# ğŸ§ª ì‹¤í—˜ ë° ê²€ì¦

## Ablation Study (ê³„íš)

ê° ì»´í¬ë„ŒíŠ¸ì˜ ê¸°ì—¬ë„ ë¶„ì„:

| ì‹¤í—˜ | êµ¬ì„± | ì˜ˆìƒ ì •í™•ë„ | ëª©ì  |
|------|------|-----------|------|
| Baseline | CNN only (ë‹¨ì¼ í”„ë ˆì„) | 75% | LSTM í•„ìš”ì„± ê²€ì¦ |
| +LSTM | CNN + LSTM | 85% | ì‹œê°„ ì •ë³´ íš¨ê³¼ |
| +Bidirectional | CNN + BiLSTM | 87% | ì–‘ë°©í–¥ íš¨ê³¼ |
| +Data Augmentation | + Aug | 89% | ì¦ê°• íš¨ê³¼ |
| +Korean Dataset | + Fine-tune | 90%+ | ë„ë©”ì¸ ì ì‘ íš¨ê³¼ |

---

## í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

### Grid Search ê³„íš

| íŒŒë¼ë¯¸í„° | í›„ë³´ ê°’ | ì„ íƒ ê¸°ì¤€ |
|---------|---------|----------|
| Sequence Length | [5, 10, 15] | ì •í™•ë„ vs ì†ë„ |
| LSTM Hidden Size | [64, 128, 256] | ì„±ëŠ¥ vs ë©”ëª¨ë¦¬ |
| LSTM Layers | [1, 2, 3] | ë³µì¡ë„ |
| Learning Rate | [1e-2, 1e-3, 1e-4] | ìˆ˜ë ´ ì†ë„ |
| Batch Size | [16, 32, 64] | GPU ë©”ëª¨ë¦¬ |
| Dropout | [0.3, 0.5, 0.7] | ê³¼ì í•© ë°©ì§€ |

---

# ğŸ“ˆ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

## í•™ìŠµ ì¤‘ ê¸°ë¡í•  ë©”íŠ¸ë¦­

**Training Metrics**:
- train_loss
- train_accuracy

**Validation Metrics**:
- val_loss
- val_accuracy
- val_precision
- val_recall
- val_f1

**Others**:
- learning_rate
- inference_time_ms

**ì‹œê°í™”**: TensorBoard

---

## ì¶”ë¡  ì‹œ ëª¨ë‹ˆí„°ë§

### API ì„œë²„ ë©”íŠ¸ë¦­

**ì„±ëŠ¥ ì§€í‘œ**:
- total_requests
- avg_inference_time_ms
- p95_latency_ms (95th percentile)
- fps
- errors

**ëª¨ë‹ˆí„°ë§ ë„êµ¬**:
- Prometheus (ë©”íŠ¸ë¦­ ìˆ˜ì§‘)
- Grafana (ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ)

---

# ğŸ”„ ë²„ì „ ê´€ë¦¬ ë° ì‹¤í—˜ ì¶”ì 

## ëª¨ë¸ ë²„ì €ë‹

**ë””ë ‰í† ë¦¬ êµ¬ì¡°**:

models/
- v1.0.0_baseline_mobilenetv3_lstm/
  - model.pth
  - config.yaml
  - metrics.json
  - training_log.txt
- v1.1.0_korean_finetuned/
- v1.2.0_optimized_torchscript/

---

## ì‹¤í—˜ ì¶”ì  (MLflow)

**ê¸°ë¡ í•­ëª©**:

Parameters:
- backbone: mobilenet_v3_small
- lstm_hidden: 128
- sequence_length: 10
- learning_rate: 1e-3

Metrics:
- val_accuracy: 0.87
- val_f1: 0.85
- inference_ms: 12.3

Artifacts:
- model íŒŒì¼
- config íŒŒì¼
- í•™ìŠµ ë¡œê·¸

---

# ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

## êµ¬í˜„ ìš°ì„ ìˆœìœ„

1. âœ… ì•„í‚¤í…ì²˜ ì„¤ê³„ ì™„ë£Œ
2. â³ ë°ì´í„° ë¡œë” êµ¬í˜„
3. â³ ëª¨ë¸ í´ë˜ìŠ¤ êµ¬í˜„
4. â³ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
5. â³ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
6. â³ API ì„œë²„ êµ¬í˜„

## ê²€ì¦ ê³„íš

- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (ê° ëª¨ë“ˆ)
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ (ì „ì²´ íŒŒì´í”„ë¼ì¸)
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
- [ ] ì‹¤ì‚¬ìš© í…ŒìŠ¤íŠ¸ (íŒ€ì›ë“¤ê³¼)

---

ì‘ì„±ì: AI ëª¨ë¸ ê°œë°œì
ë¦¬ë·°ì–´: -
ìµœì¢… ì—…ë°ì´íŠ¸: 2026-01-12
